# Trainer Configuration (Transformer-optimized)
# Contains: Transformer-typical training settings

_target_: lightning.pytorch.Trainer

# Compute / Runtime
accelerator: "gpu"
devices: 1
strategy: "auto"
precision: "16-mixed"

# Training Loop Controls
max_epochs: 5
max_steps: -1
val_check_interval: 1.0

# Performance / Stability (gradient clipping critical for Transformers)
accumulate_grad_batches: 1
gradient_clip_val: 1.0          # Much more common for Transformers
gradient_clip_algorithm: "norm"
deterministic: false
benchmark: true

# Logging / Debugging
log_every_n_steps: 50
enable_progress_bar: true
num_sanity_val_steps: 2
fast_dev_run: false
