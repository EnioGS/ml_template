# AdamW Optimizer Configuration (Transformer-optimized)
# Contains: optimizer target, Transformer-typical hyperparams

_target_: torch.optim.AdamW

# Optimizer Hyperparams (typical for Transformers)
lr: 3e-5
weight_decay: 0.01
betas: [0.9, 0.999]
eps: 1e-8
