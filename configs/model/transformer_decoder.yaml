# Transformer Decoder Configuration (GPT-like)
# Contains: LightningModule target, task config, nested backbone

_target_: src.models.transformer_module.TransformerDecoderLitModule

# Task Configuration
task: language_modeling         # language_modeling / text_generation
label_smoothing: 0.0

# Backbone Architecture (nested pattern)
backbone:
  _target_: src.modules.transformer.TransformerDecoder
  vocab_size: 50257
  d_model: 768
  n_layers: 12
  n_heads: 12
  d_ff: 3072
  dropout: 0.1
  attention_dropout: 0.1
  max_seq_len: 1024
  activation: "gelu"
  causal: true
